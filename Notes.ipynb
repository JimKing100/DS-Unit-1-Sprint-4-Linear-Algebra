{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JimKing100/DS-Unit-1-Sprint-4-Linear-Algebra/blob/master/Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjSQ6z9-QLDg",
        "colab_type": "text"
      },
      "source": [
        "### Why We Care About Linear Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHhtAWpVQW7w",
        "colab_type": "text"
      },
      "source": [
        "Because the mathematical principles behinds vectors and matrices (lists and 2D lists) will help us understand how we can tell computers how to do an insane number of calculations in a very short amount of time.\n",
        "\n",
        "Vectors: Rows, Columns, lists, arrays\n",
        "\n",
        "Matrices: tables, spreadsheets, dataframes\n",
        "\n",
        "Linear Regression: (You might remember from the intro course)\n",
        "\n",
        "Dimensionality Reduction Techniques: Principle Component Analysis (PCA) and Singular Value Decomposition (SVD)\n",
        "\n",
        "Deep Learning: Convolutional Neural Networks, (Image Recognition)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN1Lgw_cRqFI",
        "colab_type": "text"
      },
      "source": [
        "### Module 1 - Assignment\n",
        "\n",
        "Part 1 - Scalars and Vectors\n",
        "\n",
        "1) Plot a 2D vector\n",
        "\n",
        "2) Plot a 3D vector\n",
        "\n",
        "3) Scale the vectors by 1.1, 5, pi and -e\n",
        "\n",
        "4) Plot 2 vectors\n",
        "\n",
        "5) Find Va - Vb and plot\n",
        "\n",
        "6) Find Vc . Vd (dot product) (np.dot)\n",
        "\n",
        "7) Find Ve x Vf (multiplication) (np.matmul)\n",
        "\n",
        "8) Find ||Vg|| (norm) (LA.norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEsX-SzzTD3D",
        "colab_type": "text"
      },
      "source": [
        "Part 2 - Matrices\n",
        "\n",
        "1) Dimensions of Matrices (rows x columns)\n",
        "\n",
        "1a) Multiplication of Matrices - # Rows = # Columns\n",
        "\n",
        "2) Dot Product of Matrices\n",
        "\n",
        "3) Transpose Matrices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANVzmNJ7VcRs",
        "colab_type": "text"
      },
      "source": [
        "Part 3 - Square Matrices\n",
        "\n",
        "1) Identity x G\n",
        "\n",
        "2) Determinant |H|\n",
        "\n",
        "3) Inverse\n",
        "\n",
        "4) H x Inverse - Identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "divNo-VxWIer",
        "colab_type": "text"
      },
      "source": [
        "### Module 2 - Lecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw7hlBlWWQpe",
        "colab_type": "text"
      },
      "source": [
        "1) Variance - The average of the squared differences from the mean. \n",
        "\n",
        "2) Degrees of Freedom - Use 1 for sample (most cases), 0 for population.\n",
        "\n",
        "3) Standard Deviation - The square root of the variance. The average distance from the mean.\n",
        "\n",
        "4) Covariance - How changes in onevariable are associated with changes in a second variable.\n",
        "\n",
        "5) Variance-Covariance Matrix - This is matrix that compares each variable with every other variable in a dataset and returns to us variance values along the main diagonal, and covariance values everywhere else.\n",
        "\n",
        "6) Correlation Coefficent - Divide our covariance values by the product of the standard deviations of the two variables. r = cov(X,Y)/stdX x stdY, values between -1 and 1.\n",
        "\n",
        "7) Othogonality - Two vectors that are perpendicular to one another are orthogonal.\n",
        "\n",
        "8) Unit Vectors - A unit vector is any vector of \"unit length\" (1). You can turn any non-zero vector into a unit vector by dividing it by its norm (length/magnitude).\n",
        "\n",
        "9) Span - The span is the set of all possible vectors that can be created with a linear combination of two vectors.\n",
        "\n",
        "10) Linearly Dependent Vectors - Two vectors that live on the same line.\n",
        "\n",
        "11) Linearly Independent Vectors - Vectors that don't lie on the same line as each other.\n",
        "\n",
        "12) Basis - The basis of a vector space $V$ is a set of vectors that are linearly independent and that span the vector space $V$.\n",
        "\n",
        "13) Orthogonal Basis - A set of vectors that are linearly independent, span the vector space, and are orthogonal to each other. Remember that vectors are orthogonal if their dot product equals zero.\n",
        "\n",
        "14) Orthonormal Basis - a set of vectors that are linearly independent, span the vector space, are orthogonal to each other and each have unit length.\n",
        "\n",
        "15) Rank - The rank of a matrix is the dimension of the vector space spanned by its columns.\n",
        "\n",
        "16) Gaussian Elimination - a process that seeks to take any given matrix and reduce it down to what is called \"Row-Echelon form.\" \n",
        "\n",
        "17) Linear Projections -projL(w) = [(w * v)/(v*V)] * v\n",
        "\n",
        "\n"
      ]
    }
  ]
}